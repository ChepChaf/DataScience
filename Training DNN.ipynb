{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-1367280bd80c>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/chep/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/chep/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/chep/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/chep/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = mnist.train.images[0].shape[0]\n",
    "n_outputs = len(np.unique(mnist.train.labels))\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=tf.nn.elu, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"logits\", activation_fn=None)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(0.01, momentum=0.9) \n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"metric\"):\n",
    "    accuracy = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits,1))\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0  Accuracy:  (0.0, 0.9421)\n",
      "Batch:  1  Accuracy:  (0.9421, 0.9484)\n",
      "Batch:  2  Accuracy:  (0.9484, 0.953)\n",
      "Batch:  3  Accuracy:  (0.953, 0.9562)\n",
      "Batch:  4  Accuracy:  (0.9562, 0.95892)\n",
      "Batch:  5  Accuracy:  (0.95892, 0.9601167)\n",
      "Batch:  6  Accuracy:  (0.9601167, 0.96187145)\n",
      "Batch:  7  Accuracy:  (0.96187145, 0.9634125)\n",
      "Batch:  8  Accuracy:  (0.9634125, 0.96467775)\n",
      "Batch:  9  Accuracy:  (0.96467775, 0.96596)\n",
      "Batch:  10  Accuracy:  (0.96596, 0.96687275)\n",
      "Batch:  11  Accuracy:  (0.96687275, 0.967475)\n",
      "Batch:  12  Accuracy:  (0.967475, 0.9682)\n",
      "Batch:  13  Accuracy:  (0.9682, 0.96875715)\n",
      "Batch:  14  Accuracy:  (0.96875715, 0.96945333)\n",
      "Batch:  15  Accuracy:  (0.96945333, 0.969825)\n",
      "Batch:  16  Accuracy:  (0.969825, 0.97031766)\n",
      "Batch:  17  Accuracy:  (0.97031766, 0.9707556)\n",
      "Batch:  18  Accuracy:  (0.9707556, 0.97107893)\n",
      "Batch:  19  Accuracy:  (0.97107893, 0.971425)\n",
      "Batch:  20  Accuracy:  (0.971425, 0.971719)\n",
      "Batch:  21  Accuracy:  (0.971719, 0.9720182)\n",
      "Batch:  22  Accuracy:  (0.9720182, 0.9722913)\n",
      "Batch:  23  Accuracy:  (0.9722913, 0.97255)\n",
      "Batch:  24  Accuracy:  (0.97255, 0.97278)\n",
      "Batch:  25  Accuracy:  (0.97278, 0.97303843)\n",
      "Batch:  26  Accuracy:  (0.97303843, 0.9732481)\n",
      "Batch:  27  Accuracy:  (0.9732481, 0.97347856)\n",
      "Batch:  28  Accuracy:  (0.97347856, 0.97368276)\n",
      "Batch:  29  Accuracy:  (0.97368276, 0.97388)\n",
      "Batch:  30  Accuracy:  (0.97388, 0.9740613)\n",
      "Batch:  31  Accuracy:  (0.9740613, 0.97424686)\n",
      "Batch:  32  Accuracy:  (0.97424686, 0.97442424)\n",
      "Batch:  33  Accuracy:  (0.97442424, 0.9745706)\n",
      "Batch:  34  Accuracy:  (0.9745706, 0.9747057)\n",
      "Batch:  35  Accuracy:  (0.9747057, 0.97484165)\n",
      "Batch:  36  Accuracy:  (0.97484165, 0.9749757)\n",
      "Batch:  37  Accuracy:  (0.9749757, 0.9750842)\n",
      "Batch:  38  Accuracy:  (0.9750842, 0.9752)\n",
      "Batch:  39  Accuracy:  (0.9752, 0.97531)\n",
      "Batch:  40  Accuracy:  (0.97531, 0.9754195)\n",
      "Batch:  41  Accuracy:  (0.9754195, 0.97551906)\n",
      "Batch:  42  Accuracy:  (0.97551906, 0.97561395)\n",
      "Batch:  43  Accuracy:  (0.97561395, 0.97570455)\n",
      "Batch:  44  Accuracy:  (0.97570455, 0.9757978)\n",
      "Batch:  45  Accuracy:  (0.9757978, 0.9758761)\n",
      "Batch:  46  Accuracy:  (0.9758761, 0.97595316)\n",
      "Batch:  47  Accuracy:  (0.97595316, 0.9760187)\n",
      "Batch:  48  Accuracy:  (0.9760187, 0.9760898)\n",
      "Batch:  49  Accuracy:  (0.9760898, 0.976168)\n",
      "Batch:  50  Accuracy:  (0.976168, 0.9762392)\n",
      "Batch:  51  Accuracy:  (0.9762392, 0.9763058)\n",
      "Batch:  52  Accuracy:  (0.9763058, 0.9763585)\n",
      "Batch:  53  Accuracy:  (0.9763585, 0.97642034)\n",
      "Batch:  54  Accuracy:  (0.97642034, 0.97648)\n",
      "Batch:  55  Accuracy:  (0.97648, 0.9765446)\n",
      "Batch:  56  Accuracy:  (0.9765446, 0.97659826)\n",
      "Batch:  57  Accuracy:  (0.97659826, 0.9766569)\n",
      "Batch:  58  Accuracy:  (0.9766569, 0.97671187)\n",
      "Batch:  59  Accuracy:  (0.97671187, 0.9767633)\n",
      "Batch:  60  Accuracy:  (0.9767633, 0.9768164)\n",
      "Batch:  61  Accuracy:  (0.9768164, 0.9768597)\n",
      "Batch:  62  Accuracy:  (0.9768597, 0.9769016)\n",
      "Batch:  63  Accuracy:  (0.9769016, 0.97694373)\n",
      "Batch:  64  Accuracy:  (0.97694373, 0.9769846)\n",
      "Batch:  65  Accuracy:  (0.9769846, 0.9770227)\n",
      "Batch:  66  Accuracy:  (0.9770227, 0.9770612)\n",
      "Batch:  67  Accuracy:  (0.9770612, 0.97710294)\n",
      "Batch:  68  Accuracy:  (0.97710294, 0.9771377)\n",
      "Batch:  69  Accuracy:  (0.9771377, 0.9771714)\n",
      "Batch:  70  Accuracy:  (0.9771714, 0.97720706)\n",
      "Batch:  71  Accuracy:  (0.97720706, 0.9772361)\n",
      "Batch:  72  Accuracy:  (0.9772361, 0.97726715)\n",
      "Batch:  73  Accuracy:  (0.97726715, 0.97730136)\n",
      "Batch:  74  Accuracy:  (0.97730136, 0.9773347)\n",
      "Batch:  75  Accuracy:  (0.9773347, 0.97736186)\n",
      "Batch:  76  Accuracy:  (0.97736186, 0.977387)\n",
      "Batch:  77  Accuracy:  (0.977387, 0.9774167)\n",
      "Batch:  78  Accuracy:  (0.9774167, 0.9774392)\n",
      "Batch:  79  Accuracy:  (0.9774392, 0.977465)\n",
      "Batch:  80  Accuracy:  (0.977465, 0.9774901)\n",
      "Batch:  81  Accuracy:  (0.9774901, 0.9775146)\n",
      "Batch:  82  Accuracy:  (0.9775146, 0.97753733)\n",
      "Batch:  83  Accuracy:  (0.97753733, 0.9775607)\n",
      "Batch:  84  Accuracy:  (0.9775607, 0.97758234)\n",
      "Batch:  85  Accuracy:  (0.97758234, 0.9776046)\n",
      "Batch:  86  Accuracy:  (0.9776046, 0.97762644)\n",
      "Batch:  87  Accuracy:  (0.97762644, 0.9776466)\n",
      "Batch:  88  Accuracy:  (0.9776466, 0.9776674)\n",
      "Batch:  89  Accuracy:  (0.9776674, 0.9776889)\n",
      "Batch:  90  Accuracy:  (0.9776889, 0.9777088)\n",
      "Batch:  91  Accuracy:  (0.9777088, 0.9777261)\n",
      "Batch:  92  Accuracy:  (0.9777261, 0.9777441)\n",
      "Batch:  93  Accuracy:  (0.9777441, 0.97776383)\n",
      "Batch:  94  Accuracy:  (0.97776383, 0.97778)\n",
      "Batch:  95  Accuracy:  (0.97778, 0.97779477)\n",
      "Batch:  96  Accuracy:  (0.97779477, 0.97780925)\n",
      "Batch:  97  Accuracy:  (0.97780925, 0.9778245)\n",
      "Batch:  98  Accuracy:  (0.9778245, 0.9778374)\n",
      "Batch:  99  Accuracy:  (0.9778374, 0.97785)\n",
      "Batch:  100  Accuracy:  (0.97785, 0.9778644)\n",
      "Batch:  101  Accuracy:  (0.9778644, 0.9778794)\n",
      "Batch:  102  Accuracy:  (0.9778794, 0.97789323)\n",
      "Batch:  103  Accuracy:  (0.97789323, 0.97790575)\n",
      "Batch:  104  Accuracy:  (0.97790575, 0.97792)\n",
      "Batch:  105  Accuracy:  (0.97792, 0.97793394)\n",
      "Batch:  106  Accuracy:  (0.97793394, 0.97794676)\n",
      "Batch:  107  Accuracy:  (0.97794676, 0.97796017)\n",
      "Batch:  108  Accuracy:  (0.97796017, 0.97797066)\n",
      "Batch:  109  Accuracy:  (0.97797066, 0.9779827)\n",
      "Batch:  110  Accuracy:  (0.9779827, 0.97799367)\n",
      "Batch:  111  Accuracy:  (0.97799367, 0.97800535)\n",
      "Batch:  112  Accuracy:  (0.97800535, 0.9780159)\n",
      "Batch:  113  Accuracy:  (0.9780159, 0.97802716)\n",
      "Batch:  114  Accuracy:  (0.97802716, 0.97804)\n",
      "Batch:  115  Accuracy:  (0.97804, 0.9780517)\n",
      "Batch:  116  Accuracy:  (0.9780517, 0.97806156)\n",
      "Batch:  117  Accuracy:  (0.97806156, 0.9780737)\n",
      "Batch:  118  Accuracy:  (0.9780737, 0.9780832)\n",
      "Batch:  119  Accuracy:  (0.9780832, 0.97809166)\n",
      "Batch:  120  Accuracy:  (0.97809166, 0.9781017)\n",
      "Batch:  121  Accuracy:  (0.9781017, 0.97810984)\n",
      "Batch:  122  Accuracy:  (0.97810984, 0.9781195)\n",
      "Batch:  123  Accuracy:  (0.9781195, 0.97812825)\n",
      "Batch:  124  Accuracy:  (0.97812825, 0.9781376)\n",
      "Batch:  125  Accuracy:  (0.9781376, 0.97814524)\n",
      "Batch:  126  Accuracy:  (0.97814524, 0.9781535)\n",
      "Batch:  127  Accuracy:  (0.9781535, 0.9781609)\n",
      "Batch:  128  Accuracy:  (0.9781609, 0.97816896)\n",
      "Batch:  129  Accuracy:  (0.97816896, 0.97817767)\n",
      "Batch:  130  Accuracy:  (0.97817767, 0.9781855)\n",
      "Batch:  131  Accuracy:  (0.9781855, 0.97819316)\n",
      "Batch:  132  Accuracy:  (0.97819316, 0.9782)\n",
      "Batch:  133  Accuracy:  (0.9782, 0.9782067)\n",
      "Batch:  134  Accuracy:  (0.9782067, 0.9782141)\n",
      "Batch:  135  Accuracy:  (0.9782141, 0.9782213)\n",
      "Batch:  136  Accuracy:  (0.9782213, 0.9782292)\n",
      "Batch:  137  Accuracy:  (0.9782292, 0.97823477)\n",
      "Batch:  138  Accuracy:  (0.97823477, 0.9782403)\n",
      "Batch:  139  Accuracy:  (0.9782403, 0.97824645)\n",
      "Batch:  140  Accuracy:  (0.97824645, 0.9782532)\n",
      "Batch:  141  Accuracy:  (0.9782532, 0.97825986)\n",
      "Batch:  142  Accuracy:  (0.97825986, 0.9782664)\n",
      "Batch:  143  Accuracy:  (0.9782664, 0.9782729)\n",
      "Batch:  144  Accuracy:  (0.9782729, 0.97827864)\n",
      "Batch:  145  Accuracy:  (0.97827864, 0.9782836)\n",
      "Batch:  146  Accuracy:  (0.9782836, 0.9782891)\n",
      "Batch:  147  Accuracy:  (0.9782891, 0.97829527)\n",
      "Batch:  148  Accuracy:  (0.97829527, 0.97830135)\n",
      "Batch:  149  Accuracy:  (0.97830135, 0.97830665)\n",
      "Batch:  150  Accuracy:  (0.97830665, 0.9783119)\n",
      "Batch:  151  Accuracy:  (0.9783119, 0.9783171)\n",
      "Batch:  152  Accuracy:  (0.9783171, 0.97832286)\n",
      "Batch:  153  Accuracy:  (0.97832286, 0.97832793)\n",
      "Batch:  154  Accuracy:  (0.97832793, 0.9783323)\n",
      "Batch:  155  Accuracy:  (0.9783323, 0.9783378)\n",
      "Batch:  156  Accuracy:  (0.9783378, 0.97834265)\n",
      "Batch:  157  Accuracy:  (0.97834265, 0.9783481)\n",
      "Batch:  158  Accuracy:  (0.9783481, 0.97835284)\n",
      "Batch:  159  Accuracy:  (0.97835284, 0.9783575)\n",
      "Batch:  160  Accuracy:  (0.9783575, 0.9783621)\n",
      "Batch:  161  Accuracy:  (0.9783621, 0.9783667)\n",
      "Batch:  162  Accuracy:  (0.9783667, 0.97837114)\n",
      "Batch:  163  Accuracy:  (0.97837114, 0.978375)\n",
      "Batch:  164  Accuracy:  (0.978375, 0.9783806)\n",
      "Batch:  165  Accuracy:  (0.9783806, 0.97838557)\n",
      "Batch:  166  Accuracy:  (0.97838557, 0.9783898)\n",
      "Batch:  167  Accuracy:  (0.9783898, 0.97839403)\n",
      "Batch:  168  Accuracy:  (0.97839403, 0.9783982)\n",
      "Batch:  169  Accuracy:  (0.9783982, 0.9784024)\n",
      "Batch:  170  Accuracy:  (0.9784024, 0.9784064)\n",
      "Batch:  171  Accuracy:  (0.9784064, 0.978411)\n",
      "Batch:  172  Accuracy:  (0.978411, 0.978415)\n",
      "Batch:  173  Accuracy:  (0.978415, 0.97841895)\n",
      "Batch:  174  Accuracy:  (0.97841895, 0.9784234)\n",
      "Batch:  175  Accuracy:  (0.9784234, 0.9784267)\n",
      "Batch:  176  Accuracy:  (0.9784267, 0.97843105)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  177  Accuracy:  (0.97843105, 0.97843426)\n",
      "Batch:  178  Accuracy:  (0.97843426, 0.9784374)\n",
      "Batch:  179  Accuracy:  (0.9784374, 0.9784411)\n",
      "Batch:  180  Accuracy:  (0.9784411, 0.97844476)\n",
      "Batch:  181  Accuracy:  (0.97844476, 0.9784478)\n",
      "Batch:  182  Accuracy:  (0.9784478, 0.9784514)\n",
      "Batch:  183  Accuracy:  (0.9784514, 0.9784549)\n",
      "Batch:  184  Accuracy:  (0.9784549, 0.97845894)\n",
      "Batch:  185  Accuracy:  (0.97845894, 0.97846234)\n",
      "Batch:  186  Accuracy:  (0.97846234, 0.97846526)\n",
      "Batch:  187  Accuracy:  (0.97846526, 0.9784686)\n",
      "Batch:  188  Accuracy:  (0.9784686, 0.97847193)\n",
      "Batch:  189  Accuracy:  (0.97847193, 0.9784753)\n",
      "Batch:  190  Accuracy:  (0.9784753, 0.97847855)\n",
      "Batch:  191  Accuracy:  (0.97847855, 0.9784823)\n",
      "Batch:  192  Accuracy:  (0.9784823, 0.97848547)\n",
      "Batch:  193  Accuracy:  (0.97848547, 0.9784887)\n",
      "Batch:  194  Accuracy:  (0.9784887, 0.9784913)\n",
      "Batch:  195  Accuracy:  (0.9784913, 0.9784944)\n",
      "Batch:  196  Accuracy:  (0.9784944, 0.97849745)\n",
      "Batch:  197  Accuracy:  (0.97849745, 0.9785005)\n",
      "Batch:  198  Accuracy:  (0.9785005, 0.978503)\n",
      "Batch:  199  Accuracy:  (0.978503, 0.978506)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for n in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"Batch: \", n, \" Accuracy: \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(X, epsilon=0.001):\n",
    "    return (X - np.mean(X)) / np.sqrt(np.power(np.std(X), 2) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.79489515, -0.41893123,  0.65525141, -1.06344081,  1.62201579])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0.5, 1.2, 3.2, 0.0, 5.0])\n",
    "normalized(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(X, scaling=1, beta=0.1):\n",
    "    return scaling * normalized(X) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.69489515, -0.31893123,  0.75525141, -0.96344081,  1.72201579])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_normalization(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=bn_params):\n",
    "            hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "            hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "            logits = fully_connected(hidden2, n_outputs, activation_fn=tf.nn.softmax, scope=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "with tf.name_scope(\"train\"):\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    accuracy = tf.metrics.accuracy(labels=y, predictions=tf.argmax(logits, 1))\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chep/anaconda3/envs/ds/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Accuracy:  (0.0, 0.9712)\n",
      "Epoch:  1  Accuracy:  (0.9712, 0.9731)\n",
      "Epoch:  2  Accuracy:  (0.9731, 0.9752)\n",
      "Epoch:  3  Accuracy:  (0.9752, 0.976225)\n",
      "Epoch:  4  Accuracy:  (0.976225, 0.97732)\n",
      "Epoch:  5  Accuracy:  (0.97732, 0.9781833)\n",
      "Epoch:  6  Accuracy:  (0.9781833, 0.9789)\n",
      "Epoch:  7  Accuracy:  (0.9789, 0.979525)\n",
      "Epoch:  8  Accuracy:  (0.979525, 0.97992224)\n",
      "Epoch:  9  Accuracy:  (0.97992224, 0.9805)\n",
      "Epoch:  10  Accuracy:  (0.9805, 0.98087275)\n",
      "Epoch:  11  Accuracy:  (0.98087275, 0.98116666)\n",
      "Epoch:  12  Accuracy:  (0.98116666, 0.9814692)\n",
      "Epoch:  13  Accuracy:  (0.9814692, 0.98170716)\n",
      "Epoch:  14  Accuracy:  (0.98170716, 0.9819667)\n",
      "Epoch:  15  Accuracy:  (0.9819667, 0.9821625)\n",
      "Epoch:  16  Accuracy:  (0.9821625, 0.98233527)\n",
      "Epoch:  17  Accuracy:  (0.98233527, 0.9825722)\n",
      "Epoch:  18  Accuracy:  (0.9825722, 0.9827)\n",
      "Epoch:  19  Accuracy:  (0.9827, 0.982855)\n",
      "Epoch:  20  Accuracy:  (0.982855, 0.9829619)\n",
      "Epoch:  21  Accuracy:  (0.9829619, 0.98308635)\n",
      "Epoch:  22  Accuracy:  (0.98308635, 0.9831826)\n",
      "Epoch:  23  Accuracy:  (0.9831826, 0.983325)\n",
      "Epoch:  24  Accuracy:  (0.983325, 0.983376)\n",
      "Epoch:  25  Accuracy:  (0.983376, 0.98347694)\n",
      "Epoch:  26  Accuracy:  (0.98347694, 0.9835296)\n",
      "Epoch:  27  Accuracy:  (0.9835296, 0.9836)\n",
      "Epoch:  28  Accuracy:  (0.9836, 0.98366207)\n",
      "Epoch:  29  Accuracy:  (0.98366207, 0.98371667)\n",
      "Epoch:  30  Accuracy:  (0.98371667, 0.9837742)\n",
      "Epoch:  31  Accuracy:  (0.9837742, 0.983775)\n",
      "Epoch:  32  Accuracy:  (0.983775, 0.9838091)\n",
      "Epoch:  33  Accuracy:  (0.9838091, 0.9838588)\n",
      "Epoch:  34  Accuracy:  (0.9838588, 0.98388857)\n",
      "Epoch:  35  Accuracy:  (0.98388857, 0.98391664)\n",
      "Epoch:  36  Accuracy:  (0.98391664, 0.9839432)\n",
      "Epoch:  37  Accuracy:  (0.9839432, 0.9839658)\n",
      "Epoch:  38  Accuracy:  (0.9839658, 0.9840026)\n",
      "Epoch:  39  Accuracy:  (0.9840026, 0.983995)\n",
      "Epoch:  40  Accuracy:  (0.983995, 0.9839927)\n",
      "Epoch:  41  Accuracy:  (0.9839927, 0.98399764)\n",
      "Epoch:  42  Accuracy:  (0.98399764, 0.9839814)\n",
      "Epoch:  43  Accuracy:  (0.9839814, 0.9839841)\n",
      "Epoch:  44  Accuracy:  (0.9839841, 0.98400664)\n",
      "Epoch:  45  Accuracy:  (0.98400664, 0.9840043)\n",
      "Epoch:  46  Accuracy:  (0.9840043, 0.9840277)\n",
      "Epoch:  47  Accuracy:  (0.9840277, 0.98405623)\n",
      "Epoch:  48  Accuracy:  (0.98405623, 0.9840816)\n",
      "Epoch:  49  Accuracy:  (0.9840816, 0.984104)\n",
      "Epoch:  50  Accuracy:  (0.984104, 0.98411375)\n",
      "Epoch:  51  Accuracy:  (0.98411375, 0.9841269)\n",
      "Epoch:  52  Accuracy:  (0.9841269, 0.9841415)\n",
      "Epoch:  53  Accuracy:  (0.9841415, 0.9841389)\n",
      "Epoch:  54  Accuracy:  (0.9841389, 0.9841509)\n",
      "Epoch:  55  Accuracy:  (0.9841509, 0.98415536)\n",
      "Epoch:  56  Accuracy:  (0.98415536, 0.9841544)\n",
      "Epoch:  57  Accuracy:  (0.9841544, 0.98416895)\n",
      "Epoch:  58  Accuracy:  (0.98416895, 0.9841797)\n",
      "Epoch:  59  Accuracy:  (0.9841797, 0.98417336)\n",
      "Epoch:  60  Accuracy:  (0.98417336, 0.98418033)\n",
      "Epoch:  61  Accuracy:  (0.98418033, 0.9842129)\n",
      "Epoch:  62  Accuracy:  (0.9842129, 0.9842254)\n",
      "Epoch:  63  Accuracy:  (0.9842254, 0.9842328)\n",
      "Epoch:  64  Accuracy:  (0.9842328, 0.9842215)\n",
      "Epoch:  65  Accuracy:  (0.9842215, 0.98422426)\n",
      "Epoch:  66  Accuracy:  (0.98422426, 0.9842164)\n",
      "Epoch:  67  Accuracy:  (0.9842164, 0.98423237)\n",
      "Epoch:  68  Accuracy:  (0.98423237, 0.9842391)\n",
      "Epoch:  69  Accuracy:  (0.9842391, 0.9842271)\n",
      "Epoch:  70  Accuracy:  (0.9842271, 0.98422396)\n",
      "Epoch:  71  Accuracy:  (0.98422396, 0.9842069)\n",
      "Epoch:  72  Accuracy:  (0.9842069, 0.9842)\n",
      "Epoch:  73  Accuracy:  (0.9842, 0.9841824)\n",
      "Epoch:  74  Accuracy:  (0.9841824, 0.98418266)\n",
      "Epoch:  75  Accuracy:  (0.98418266, 0.98417366)\n",
      "Epoch:  76  Accuracy:  (0.98417366, 0.9841727)\n",
      "Epoch:  77  Accuracy:  (0.9841727, 0.9841705)\n",
      "Epoch:  78  Accuracy:  (0.9841705, 0.98417217)\n",
      "Epoch:  79  Accuracy:  (0.98417217, 0.9841713)\n",
      "Epoch:  80  Accuracy:  (0.9841713, 0.98417777)\n",
      "Epoch:  81  Accuracy:  (0.98417777, 0.9841866)\n",
      "Epoch:  82  Accuracy:  (0.9841866, 0.98418915)\n",
      "Epoch:  83  Accuracy:  (0.98418915, 0.9841988)\n",
      "Epoch:  84  Accuracy:  (0.9841988, 0.9842082)\n",
      "Epoch:  85  Accuracy:  (0.9842082, 0.98421395)\n",
      "Epoch:  86  Accuracy:  (0.98421395, 0.9842057)\n",
      "Epoch:  87  Accuracy:  (0.9842057, 0.9842114)\n",
      "Epoch:  88  Accuracy:  (0.9842114, 0.9842135)\n",
      "Epoch:  89  Accuracy:  (0.9842135, 0.98422664)\n",
      "Epoch:  90  Accuracy:  (0.98422664, 0.9842374)\n",
      "Epoch:  91  Accuracy:  (0.9842374, 0.9842511)\n",
      "Epoch:  92  Accuracy:  (0.9842511, 0.98426235)\n",
      "Epoch:  93  Accuracy:  (0.98426235, 0.98426807)\n",
      "Epoch:  94  Accuracy:  (0.98426807, 0.9842579)\n",
      "Epoch:  95  Accuracy:  (0.9842579, 0.98426145)\n",
      "Epoch:  96  Accuracy:  (0.98426145, 0.9842742)\n",
      "Epoch:  97  Accuracy:  (0.9842742, 0.9842745)\n",
      "Epoch:  98  Accuracy:  (0.9842745, 0.98428583)\n",
      "Epoch:  99  Accuracy:  (0.98428583, 0.984293)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, is_training: True})\n",
    "        print (\"Epoch: \", epoch, \" Accuracy: \", sess.run(accuracy, feed_dict={X:mnist.test.images, y:mnist.test.labels, is_training: False}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
